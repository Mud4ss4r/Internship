{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44bdd8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name : Sayyed Mudassar Iqbal Shakil Ahmed\n",
    "# Batch : DSG2404\n",
    "# Subject : BeautifulSoup Assignment Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d2cb28",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install bs4\n",
    "!pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51377e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15dac715",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Answer 1\n",
    "page1 = requests.get('https://www.imdb.com/search/title/?groups=top_100&sort=user_rating,desc')\n",
    "page1\n",
    "\n",
    "#The requests is giving error in the range of 400-499, which means client side error.\n",
    "#Upon checking IMDB robots.txt file, everything is disallowed over there, hence cant extract data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02be35e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Answer 2\n",
    "page = requests.get('https://www.patreon.com/coreyms')\n",
    "soup = BeautifulSoup(page.content)\n",
    "soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a145b71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "head = soup.find('span',class_=\"sc-1cvoi1y-0 hxhWXn\")\n",
    "head\n",
    "# I tried all the classes to extract the asked data, nothing worked. this question also looks doubtfull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf5fbc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Answer 3\n",
    "page = requests.get('https://www.nobroker.in/property/sale/bangalore/multiple?searchParam=W3sibGF0IjoxMi45NzgzNjkyLCJsb24iOjc3LjY0MDgzNTYsInBsYWNlSWQiOiJDaElKa1FOM0dLUVdyanNSTmhCUUpyaEdEN1UiLCJwbGFjZU5hbWUiOiJJbmRpcmFuYWdhciIsInNob3dNYXAiOmZhbHNlfSx7ImxhdCI6MTIuOTMwNzczNSwibG9uIjo3Ny41ODM4MzAyLCJwbGFjZUlkIjoiQ2hJSjJkZGxaNWdWcmpzUmgxQk9BYWYtb3JzIiwicGxhY2VOYW1lIjoiSmF5YW5hZ2FyIiwic2hvd01hcCI6ZmFsc2V9LHsibGF0IjoxMi45OTgxNzMyLCJsb24iOjc3LjU1MzA0NDU5OTk5OTk5LCJwbGFjZUlkIjoiQ2hJSnhmVzREUE05cmpzUktzTlRHLTVwX1FRIiwicGxhY2VOYW1lIjoiUmFqYWppbmFnYXIiLCJzaG93TWFwIjpmYWxzZX1d&isMetro=false')\n",
    "soup = BeautifulSoup(page.content)\n",
    "soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e3cec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = []\n",
    "location = []\n",
    "emi = []\n",
    "price = []\n",
    "\n",
    "for i in soup.find_all('h2',class_=\"heading-6 flex items-center font-semi-bold m-0\"):\n",
    "    titles.append(i.text)\n",
    "\n",
    "\n",
    "for j in soup.find_all('div',class_=\"mt-0.5p overflow-hidden overflow-ellipsis whitespace-nowrap max-w-70 text-gray-light leading-4 po:mb-0.1p po:max-w-95\"):\n",
    "    location.append(j.text)\n",
    "\n",
    "\n",
    "for k in soup.find_all('h2',class_=\"font-semi-bold heading-6\"):\n",
    "    emi.append(k.text)\n",
    "\n",
    "\n",
    "for l in soup.find_all('h2',class_=\"heading-6 flex items-center font-semi-bold m-0\"):\n",
    "    price.append(l.text)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f49887",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in soup.find_all('div',class_=\"font-semi-bold heading-6\"):\n",
    "    emi.append(k.text)\n",
    "emi\n",
    "#This should be the method, but the HTML is complex on the website, like they have added price and EMI text under a border type complex class\n",
    "#Which was not covered in beautiful soup session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e329aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Answer 4 \n",
    "\n",
    "url = 'https://www.bewakoof.com/bestseller?sort=popular'\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "products = soup.find_all('div', class_='productCardBox')\n",
    "\n",
    "product_names = []\n",
    "prices = []\n",
    "image_urls = []\n",
    "\n",
    "for product in products[:10]:\n",
    "    name = product.find('h2',class_=\"clr-shade4 h3-p-name   undefined false  \").text\n",
    "    price = product.find('div',class_=\"discountedPriceText clr-p-black   false  \").text\n",
    "    image_url = product.find('img',class_=\"productImgTag\")['data-src']\n",
    "    \n",
    "    product_names.append(name)\n",
    "    prices.append(price)\n",
    "    image_urls.append(image_url)\n",
    "\n",
    "df = pd.DataFrame({'Product Name': product_names, 'Price': prices, 'Image URL': image_urls})\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e1e958",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Answer 5\n",
    "url = 'https://www.cnbc.com/world/?region=world'\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "headings = []\n",
    "dates = []\n",
    "links = []\n",
    "\n",
    "for i in soup.find_all('div',class_=\"RiverHeadline-headline RiverHeadline-hasThumbnail\"):\n",
    "    headings.append(i.text)\n",
    "\n",
    "for j in soup.find_all('span',class_=\"RiverByline-datePublished):\n",
    "    dates.append(j.text)\n",
    "\n",
    "for j in soup.find_all('div',class_=\"RiverHeadline-headline RiverHeadline-hasThumbnail\":\n",
    "    links.append(k.text)\n",
    "\n",
    "df = pd.DataFrame({'Heading': headings, 'Date': dates, 'Link': links})\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408a5647",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Answer 6\n",
    "url = 'https://www.keaipublishing.com/en/journals/artificial-intelligence-in-agriculture/most-downloaded-articles/'\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "titles = []\n",
    "dates = []\n",
    "authors = []\n",
    "\n",
    "for i in soup.find_all('h2',class_=\"h5 article-title\"):\n",
    "    titles.append(i.text)\n",
    "    \n",
    "for i in soup.find_all('p',class_=\"article-date\"):\n",
    "    dates.append(i.text)\n",
    "    \n",
    "for i in soup.find_all('p',class_=\"article-authors\"):\n",
    "    authors.append(i.text)\n",
    "    \n",
    "\n",
    "df = pd.DataFrame({'Title': titles, 'Date': dates, 'Author': authors})\n",
    "print(df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
